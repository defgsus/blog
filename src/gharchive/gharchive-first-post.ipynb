{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b6a0e4e",
   "metadata": {},
   "source": [
    "# hide\n",
    "title: Walking through the Github archive\n",
    "tags: data github\n",
    "enable: plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c7252f",
   "metadata": {},
   "source": [
    "There are addictions to all kinds of things. Currently, i'm addicted to generate timeseries of all kinds of public data. I just finished setting up a [data archive](https://github.com/defgsus/bahn-api-history) for a couple of API responses from the [Deutsche Bahn](https://api.deutschebahn.com) which was a actually just a procrastination tactic to pull myself away from the refactoring of the [office schedule data archive](https://github.com/defgsus/office-schedule-data) which is quite elaborate. More procrastination followed and i started using the [github search](https://github.com/search?o=desc&q=data+archive&s=updated&type=Repositories) for other repositories that collect historic data. There are quite a few interesting ones. Not long it took to stumble across the [github archive](http://www.gharchive.org/) of which i hadn't heared before. \n",
    "\n",
    "The github archive is a bunch of compressed, new-line delimited json files, containing all the [events](https://docs.github.com/en/developers/webhooks-and-events/events/github-event-types) of the github events API. Well, maybe not entirely ***all*** requests (there are statements about missing events in the [discussions](https://github.com/igrigorik/gharchive.org/issues)) but enough to make people stumble. \n",
    "\n",
    "To get an overview of the pure amount of data, download a single hour of each year since availability (Feb. 2011):\n",
    "\n",
    "```\n",
    "$ wget https://data.gharchive.org/20{11..21}-03-01-15.json.gz\n",
    "```\n",
    "\n",
    "The numbers are from a horror movie!\n",
    "```\n",
    "$ ls -l\n",
    "    450433 Apr  2  2018 2011-03-01-15.json.gz\n",
    "   2329840 Apr  2  2018 2012-03-01-15.json.gz\n",
    "   2648943 Apr  2  2018 2013-03-01-15.json.gz\n",
    "   3369069 Apr  2  2018 2014-03-01-15.json.gz\n",
    "   6365599 Apr  2  2018 2015-03-01-15.json.gz\n",
    "  21268062 Apr  2  2018 2016-03-01-15.json.gz\n",
    "  25974997 Apr  2  2018 2017-03-01-15.json.gz\n",
    "  37577449 Apr  2  2018 2018-03-01-15.json.gz\n",
    "  42593332 Mär  1  2019 2019-03-01-15.json.gz\n",
    "  35538137 Mär  1  2020 2020-03-01-15.json.gz\n",
    " 104531294 Mär  1  2021 2021-03-01-15.json.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96e83b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3759\n",
      "175437\n",
      "868835\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "print((450_000 * 24 * 365) // 2**20)\n",
    "print((21_000_000 * 24 * 365) // 2**20)\n",
    "print((104_000_000 * 24 * 365) // 2**20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7686107c",
   "metadata": {},
   "source": [
    "While the files for year 2011 might fit into 3 Gigabytes, 2016 will have more than a 100 Gb and 2021 is probably more than 500 Gb!\n",
    "\n",
    "So well, i listened to the [The Changelog episode #144](https://changelog.com/podcast/144) as suggested by gharchive.org and the author [Ilya Grigorik](https://github.com/igrigorik) was obviously struggling with the same problems. The solution at the time was to put everything on Google BigQuery. As Ilya states in the podcast: a *BigQuery* through the whole dataset takes between 1 and 10 seconds, while just reading all the files from disk takes an hour.\n",
    "\n",
    "Now, i'm getting older and my not-to-do list is growing. These times, whenever i see Google BigQuery or Google Colab Notebook, i skip the link and look for other ways to access what i want. It may just be stubbornness but that's how it is. Google APIs are blocked in my browser and i'm not in the mood to allow them for recreational purposes. \n",
    "\n",
    "Let's parse this data ourselves. How difficult can it be?\n",
    "\n",
    "I have this server with about 1.7Tb of harddisk space left so i'm downloading years 2018 to 2021 (until early November):\n",
    "\n",
    "```\n",
    "du -h\n",
    "175G\t./2018\n",
    "253G\t./2019\n",
    "419G\t./2020\n",
    "443G\t./2021\n",
    "1,3T\t.\n",
    "```\n",
    "\n",
    "Monstrous! The download took 4 hours or so, the server has a *good* connection. In the meantime i created some [utility code](https://github.com/defgsus/gharchive-stats) that i can run on the server to *bucket* the data for further processing.\n",
    "\n",
    "As an example let's look at the `WatchEvent` which represents the *starring* of a repository. Here's the first recorded WatchEvent from 2018:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"7044401123\",\n",
    "  \"type\": \"WatchEvent\",\n",
    "  \"actor\": {\n",
    "    \"id\": 1710912,\n",
    "    \"login\": \"yangwenmai\",\n",
    "    \"display_login\": \"yangwenmai\",\n",
    "    \"gravatar_id\": \"\",\n",
    "    \"url\": \"https://api.github.com/users/yangwenmai\",\n",
    "    \"avatar_url\": \"https://avatars.githubusercontent.com/u/1710912?\"\n",
    "  },\n",
    "  \"repo\": {\n",
    "    \"id\": 75951828,\n",
    "    \"name\": \"wainshine/Chinese-Names-Corpus\",\n",
    "    \"url\": \"https://api.github.com/repos/wainshine/Chinese-Names-Corpus\"\n",
    "  },\n",
    "  \"payload\": {\n",
    "    \"action\": \"started\"\n",
    "  },\n",
    "  \"public\": true,\n",
    "  \"created_at\": \"2018-01-01T00:00:02Z\"\n",
    "}\n",
    "```\n",
    "\n",
    "The bucketing script produces a CSV like this:\n",
    "\n",
    "| date                 | user       | repo                                   | action   |   events |\n",
    "|:---------------------|:-----------|:---------------------------------------|:---------|---------:|\n",
    "| 2018-01-01T00:00:00Z | yangwenmai | wainshine/Chinese-Names-Corpus         | started  |        1 |\n",
    "\n",
    "The date is *floored* to the nearest hour and every occurence of an event with the same `date`, `user`, `repo` and `action` are bucketed into a single line, increasing the `events` counter. \n",
    "\n",
    "The conversion of **all the WatchEvents in 2018**\n",
    " - took eleven hours\n",
    " - produced a CSV with\n",
    "   - 36,585,735 rows\n",
    "   - and a size of 628,047,892 bytes (compressed with gzip)\n",
    "\n",
    "Now, that's not a CSV you can just open but at least i can copy it from the server to my local system.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68314ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"7044401123\",\n",
      "  \"type\": \"WatchEvent\",\n",
      "  \"actor\": {\n",
      "    \"id\": 1710912,\n",
      "    \"login\": \"yangwenmai\",\n",
      "    \"display_login\": \"yangwenmai\",\n",
      "    \"gravatar_id\": \"\",\n",
      "    \"url\": \"https://api.github.com/users/yangwenmai\",\n",
      "    \"avatar_url\": \"https://avatars.githubusercontent.com/u/1710912?\"\n",
      "  },\n",
      "  \"repo\": {\n",
      "    \"id\": 75951828,\n",
      "    \"name\": \"wainshine/Chinese-Names-Corpus\",\n",
      "    \"url\": \"https://api.github.com/repos/wainshine/Chinese-Names-Corpus\"\n",
      "  },\n",
      "  \"payload\": {\n",
      "    \"action\": \"started\"\n",
      "  },\n",
      "  \"public\": true,\n",
      "  \"created_at\": \"2018-01-01T00:00:02Z\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "import csv\n",
    "import gzip\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with gzip.open(Path(\"~/prog/data/gharchive/2018/2018-01-01-0.json.gz\").expanduser()) as fp:\n",
    "    for line in fp.readlines():\n",
    "        event = json.loads(line)\n",
    "        if event[\"type\"] == \"WatchEvent\":\n",
    "            print(json.dumps(event, indent=2))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18b1631",
   "metadata": {},
   "source": [
    "Python's generators are quite useful to walk through a CSV or ndjson file which might not fit into memory in it's entirety. Let's measure the time it takes to find all of *yangwenmai*'s `WatchEvent`s with vanilla python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "125df459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 101.20 seconds\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import gzip\n",
    "import time\n",
    "\n",
    "# per line iterator through the gzipped csv\n",
    "def iter_lines():\n",
    "    with gzip.open(\"../../../gharchive-stats/server-all/2018/watch_h.csv.gz\", \"rt\") as fp:\n",
    "        yield from fp.readlines()\n",
    "\n",
    "rows = []\n",
    "start_time = time.time()\n",
    "\n",
    "# filter for user\n",
    "for row in csv.DictReader(iter_lines()):\n",
    "    if row[\"user\"] == \"yangwenmai\":\n",
    "        rows.append(row)\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print(f\"took {duration:.2f} seconds\")\n",
    "\n",
    "# convert to pandas\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43b0fc6",
   "metadata": {},
   "source": [
    "So, a 100 seconds, simply to parse through the CSV, no matter what we are actually looking for. This would probably lead to a query time of 10 minutes for the whole dataset. Far from optimal but it requires no complicated stuff, google accounts or lots of memory. \n",
    "\n",
    "For reasons, i quite like [Elastisearch](https://www.elastic.co/elasticsearch/) so i move the CSV file into an elasticsearch index which took 90 minutes and resulted in an index of 4.2 Gb size. Repeating the above query using [elastipy](https://elastipy.readthedocs.io/en/latest/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "22faf848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 20 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>timestamp_hour</th>\n",
       "      <th>timestamp_weekday</th>\n",
       "      <th>user</th>\n",
       "      <th>repo</th>\n",
       "      <th>action</th>\n",
       "      <th>events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-05-25T23:00:00</td>\n",
       "      <td>23</td>\n",
       "      <td>5 Friday</td>\n",
       "      <td>yangwenmai</td>\n",
       "      <td>developer-learning/learning-kubernetes</td>\n",
       "      <td>started</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-05-26T09:00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>6 Saturday</td>\n",
       "      <td>yangwenmai</td>\n",
       "      <td>danistefanovic/build-your-own-x</td>\n",
       "      <td>started</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-05-27T03:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>0 Sunday</td>\n",
       "      <td>yangwenmai</td>\n",
       "      <td>yongman/tidis</td>\n",
       "      <td>started</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-05-28T02:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1 Monday</td>\n",
       "      <td>yangwenmai</td>\n",
       "      <td>muesli/kmeans</td>\n",
       "      <td>started</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-05-28T06:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>1 Monday</td>\n",
       "      <td>yangwenmai</td>\n",
       "      <td>appoptics/appoptics-apm-go</td>\n",
       "      <td>started</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>2018-11-28T14:00:00</td>\n",
       "      <td>14</td>\n",
       "      <td>3 Wednesday</td>\n",
       "      <td>yangwenmai</td>\n",
       "      <td>mholt/archiver</td>\n",
       "      <td>started</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>2018-11-30T02:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>5 Friday</td>\n",
       "      <td>yangwenmai</td>\n",
       "      <td>henrylee2cn/aster</td>\n",
       "      <td>started</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>2018-12-11T14:00:00</td>\n",
       "      <td>14</td>\n",
       "      <td>2 Tuesday</td>\n",
       "      <td>yangwenmai</td>\n",
       "      <td>developer-learning/telegram-bot-go</td>\n",
       "      <td>started</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>2018-12-13T00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>4 Thursday</td>\n",
       "      <td>yangwenmai</td>\n",
       "      <td>bragfoo/TiPrometheus</td>\n",
       "      <td>started</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>2018-12-17T01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1 Monday</td>\n",
       "      <td>yangwenmai</td>\n",
       "      <td>cmu-db/ottertune</td>\n",
       "      <td>started</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>501 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               timestamp  timestamp_hour timestamp_weekday        user  \\\n",
       "0    2018-05-25T23:00:00              23          5 Friday  yangwenmai   \n",
       "1    2018-05-26T09:00:00               9        6 Saturday  yangwenmai   \n",
       "2    2018-05-27T03:00:00               3          0 Sunday  yangwenmai   \n",
       "3    2018-05-28T02:00:00               2          1 Monday  yangwenmai   \n",
       "4    2018-05-28T06:00:00               6          1 Monday  yangwenmai   \n",
       "..                   ...             ...               ...         ...   \n",
       "496  2018-11-28T14:00:00              14       3 Wednesday  yangwenmai   \n",
       "497  2018-11-30T02:00:00               2          5 Friday  yangwenmai   \n",
       "498  2018-12-11T14:00:00              14         2 Tuesday  yangwenmai   \n",
       "499  2018-12-13T00:00:00               0        4 Thursday  yangwenmai   \n",
       "500  2018-12-17T01:00:00               1          1 Monday  yangwenmai   \n",
       "\n",
       "                                       repo   action events  \n",
       "0    developer-learning/learning-kubernetes  started      1  \n",
       "1           danistefanovic/build-your-own-x  started      1  \n",
       "2                             yongman/tidis  started      1  \n",
       "3                             muesli/kmeans  started      1  \n",
       "4                appoptics/appoptics-apm-go  started      1  \n",
       "..                                      ...      ...    ...  \n",
       "496                          mholt/archiver  started      1  \n",
       "497                       henrylee2cn/aster  started      1  \n",
       "498      developer-learning/telegram-bot-go  started      1  \n",
       "499                    bragfoo/TiPrometheus  started      1  \n",
       "500                        cmu-db/ottertune  started      1  \n",
       "\n",
       "[501 rows x 7 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elastipy import Search\n",
    "\n",
    "response = (Search(\"gharchive-watch-2018\")\n",
    " .term(\"user\", \"yangwenmai\")\n",
    " .size(1000)\n",
    " .execute()\n",
    ")\n",
    "print(\"took\", response[\"took\"], \"ms\")\n",
    "pd.DataFrame(response.documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba25525",
   "metadata": {},
   "source": [
    "Yes! Praise \"Bob!\". Who needs the google cloud? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ca3d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
