{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1279818",
   "metadata": {},
   "source": [
    "# hide\n",
    "title: There's a new parking scraper in town\n",
    "enable: plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf2cad1",
   "metadata": {},
   "source": [
    "... just, that new scraper is [mine]({% post_url 2021/2021-04-08-one-year-parking %}). There are people doing this some years longer! So let me instead introduce [parkendd.de](https://parkendd.de/) by [Offenes Dresden](https://github.com/offenesdresden). It's [all open](https://github.com/offenesdresden/ParkAPI/) and the archive is available [for download](https://parkendd.de/dumps/).\n",
    "\n",
    "This post is similar to some of my other data investigation posts in the sense that i simply start coding and see what comes out of it. For a change, all code is included so you do not need to check the jupyter notebook. \n",
    "\n",
    "Currently (end of 2021) data from 2015 to 2020 is packaged into a big tar.xz file, which i will convert to tar.gz because it's easier to read in python. \n",
    "\n",
    "```\n",
    "wget https://parkendd.de/dumps/Archive.tar.xz\n",
    "xz -dc Archive.tar.xz | gzip -cf9 > parkapi-2020.tar.gz\n",
    "```\n",
    "\n",
    "The xz compression actually seems to be a good choice because the filesize expands from 200 to 500 megabytes with gz. Not a problematic number, though. However, the uncompressed tar file is about 3.6 Gigabytes. I want to use [pandas](https://pandas.pydata.org) and experience tells me that loading a Gigabyte csv will usually not fit into memory. Even if it does, all operations that *copy* data will eventually kill the python kernel. \n",
    "\n",
    "So, i'll iterate through all files in the archive - each representing one parking lot per year - resample them to averaged **1 hour buckets** and gradually merge them into a single DataFrame. I want to look at years 2016 to 2020, so that's about 44,000 hour steps for a 100+ paring lots which should fit into anyone's memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a827c215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tarfile\n",
    "import codecs\n",
    "import re\n",
    "from typing import Generator, Tuple, Union, Optional, Callable\n",
    "\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "pd.options.display.max_columns = 30\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "plotly.templates.default = \"plotly_dark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1b1e79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def iter_archive_dataframes(\n",
    "    filename: Union[str, Path],\n",
    "    resampling: str = \"1h\",\n",
    ") -> Generator[Tuple[str, pd.DataFrame], None, None]:\n",
    "    \n",
    "    # tarfile does handle the gzip automatically\n",
    "    with tarfile.open(filename) as tfp:\n",
    "        \n",
    "        # build map of lot_id to available csv filenames \n",
    "        #   i ignore 2015 since it's incomplete\n",
    "        lot_id_filenames = dict()\n",
    "        for filename in sorted(tfp.getnames()):\n",
    "            if \"backup\" not in filename:\n",
    "                match = re.match(\"(.*)-(20\\d\\d).csv\", filename)\n",
    "                if match:\n",
    "                    lot_id, year = match.groups()\n",
    "                    if year != \"2015\":\n",
    "                        lot_id_filenames.setdefault(lot_id, []).append(filename)\n",
    "        \n",
    "        # for each lot\n",
    "        for lot_id, filenames in lot_id_filenames.items():\n",
    "            # if we have years 2016 - 2020\n",
    "            if len(filenames) == 5:\n",
    "                # build one DataFrame, resampled to 1 hour\n",
    "                dfs = []\n",
    "                for filename in filenames:\n",
    "                    fp = tfp.extractfile(filename)\n",
    "                    dfs.append(pd.read_csv(\n",
    "                        codecs.getreader(\"utf-8\")(fp), \n",
    "                        names=[\"date\", \"free\"]\n",
    "                    ))\n",
    "                df = pd.concat(dfs, axis=0)\n",
    "                df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "                try:\n",
    "                    df = df.set_index(\"date\").resample(resampling).mean()\n",
    "                    yield lot_id, df\n",
    "                except:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db10924",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "archive_file = Path(\"~/prog/data/parking/parkapi-2020.tar.gz\").expanduser()\n",
    "table_file = Path(\"~/prog/data/parking/parkapi-2020-1h.csv\").expanduser()\n",
    "\n",
    "if not table_file.exists():\n",
    "    big_df = None\n",
    "    for lot_id, df in tqdm(iter_archive_dataframes(archive_file)):\n",
    "        df[\"lot_id\"] = lot_id\n",
    "        df = df.reset_index().set_index([\"date\", \"lot_id\"])\n",
    "        if big_df is None:\n",
    "            big_df = df\n",
    "        else:\n",
    "            # append rows and sort by date\n",
    "            big_df = pd.concat([big_df, df]).sort_index()\n",
    "    \n",
    "    # x = lot_id, y = date\n",
    "    big_df = big_df.unstack(\"lot_id\")\n",
    "    # drop the \"free\" label from columns, just keep lot_id\n",
    "    big_df.columns = big_df.columns.droplevel()\n",
    "    # store\n",
    "    big_df.to_csv(table_file)\n",
    "\n",
    "else:\n",
    "    # read the file if it was already created\n",
    "    big_df = pd.read_csv(table_file)\n",
    "    big_df[\"date\"] = pd.to_datetime(big_df[\"date\"])\n",
    "    big_df.set_index(\"date\", inplace=True)\n",
    "    big_df.columns.name = \"lot_id\"\n",
    "    \n",
    "big_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f745dc72",
   "metadata": {},
   "source": [
    "Don't mind all the *NaN*s, the city of Aalborg is not scraped throughout the whole period. But Oldenburg seems to look good. Without further number crunching let's do a quick interactive plot, resampled to 1 week buckets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcde41e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(big_df\n",
    " .resample(\"1w\").mean()\n",
    " .round()  # the round saves about 300 Kb of javascript code \n",
    " .plot(\n",
    "     title=f\"average number of free spaces per week ({big_df.shape[1]} lots)\", \n",
    "     labels={\"value\": \"number of free spaces\", \"date\": \"week\"}\n",
    " )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea9d59e",
   "metadata": {},
   "source": [
    "As usual, you can drag and zoom, and hide the inidividual lots on the right side (doubleclick to hide all except one).\n",
    "\n",
    "Now that i'm actually able to look at parking data predating this stupid covid pandemic i'll pose two simple research questions\n",
    "- *Is the lockdown around Germany in beginning of 2020 visible in the parking lot occupation data?* \n",
    "- *Has anything in the parking behaviour significantly changed compared to before?* \n",
    "\n",
    "First of all, when checking the plots above, a few cities have big chunks of missing data, Aalborg for example. It's a shame but i'll exclude them. Moreover, there are smaller gaps. Sometimes it happens that the number of free spaces listed on a website gets stuck, or is not listed at all, while other lots on the same site work fine. I'll count the number of times that the average value does not change during three days. Specifically since year 2018:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3bf1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    big_df[(big_df.index >= \"2018-01-01\")]\n",
    "    .resample(\"1d\").mean()\n",
    "    .replace(np.nan, 0)  # treat missing values as zero\n",
    ")\n",
    "num_equal_days = ((df == df.shift(1)) & (df == df.shift(2))).astype(int).sum()\n",
    "num_equal_days.sort_values().plot.bar(\n",
    "    title=\"Number of times that 3 consecutive days have unchanged number of free spaces\",\n",
    "    height=600,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb41926",
   "metadata": {},
   "source": [
    "By visual inspection and comparison with the plot on top i decide to cut everything above 100, and also remove the Zurich lot because it misses data exactly at the time in question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91b6dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df = big_df.loc[:, (num_equal_days <= 100) & (big_df.columns != \"zuerichparkgarageamcentral\")]\n",
    "big_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5111fe31",
   "metadata": {},
   "source": [
    "Okay, 53 lots remain. Now it would be great to *normalize* each lot using the total capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50bce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d423862b",
   "metadata": {},
   "source": [
    "Ah, well, the congress center in Dresden probably does not had 26 thousand spaces. I'll first clamp the dataframe to, let's say, 2000, just to remove the most obvious outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f75932",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df = big_df.clip(0, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbd9307",
   "metadata": {},
   "source": [
    "and then ask the ParkAPI for more precise values. The endpoint is `https://api.parkendd.de/<City>` which returns static and live data for each lot per city:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce396f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CITIES = [\"Aarhus\", \"Dresden\", \"Freiburg\", \"Ingolstadt\", \"Luebeck\"]\n",
    "lot_infos = dict()\n",
    "for city in CITIES:\n",
    "    response = requests.get(f\"https://api.parkendd.de/{city}\")\n",
    "    for lot in response.json()[\"lots\"]:\n",
    "        lot[\"city\"] = city\n",
    "        lot_infos[lot[\"id\"]] = lot\n",
    "\n",
    "lot_infos[\"dresdenkongresszentrum\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37274a6",
   "metadata": {},
   "source": [
    "Well 26,000 was only two magnitudes above the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd08cd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "lot_infos[\"luebeckbackbord\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b6c9c2",
   "metadata": {},
   "source": [
    "Lübeck does not provide a total value. The website that is scraped can be determined from the [geojson file](https://raw.githubusercontent.com/offenesdresden/ParkAPI/master/park_api/cities/Luebeck.geojson) of the Lübeck-scraper (or from `https://api.parkendd.de/`). It actually seems to be [offline](https://www.kwl-luebeck.de/parken/aktuelle-parkplatzbelegung) right now. So i'll use the official numbers if present and the maximum free value otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6d5676",
   "metadata": {},
   "outputs": [],
   "source": [
    "official_capacity = pd.Series(\n",
    "    big_df.columns.map(lambda c: lot_infos[c][\"total\"] or None), \n",
    "    index=big_df.columns\n",
    ").dropna()\n",
    "\n",
    "capacity = big_df.max()\n",
    "capacity[official_capacity.index] = official_capacity\n",
    "\n",
    "# lot occupation in range [0, 1]\n",
    "occupied = 1. - (big_df / capacity).clip(0, 1)\n",
    "\n",
    "(occupied\n",
    " .groupby(lambda c: lot_infos[c][\"city\"], axis=1).mean()\n",
    " .resample(\"1m\").mean() * 100.\n",
    ").plot(\n",
    "    title=\"Average lot occupation per month and city\",\n",
    "    labels={\"value\": \"occupation percentage\", \"date\": \"month\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d9041f",
   "metadata": {},
   "source": [
    "Alright. There it is. A pretty obvious dent! With the least occupation during April 2020. That's how i remember it. Kids skating on empty parking lots, no planes in the sky, no stupid shops selling useless things.\n",
    "\n",
    "For the interested, here's the same plot for each lot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4180cb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "(occupied.resample(\"1m\").mean() * 100.).round().plot(\n",
    "    title=\"Average lot occupation per month and lot\",\n",
    "    labels={\"value\": \"occupation percentage\", \"date\": \"month\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3517aad",
   "metadata": {},
   "source": [
    "There are more ways of looking at the occupation data. Instead of calculating the average for each week we can build a histogram of the occupation values. This shows all levels of occupation during each week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609c0601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(\n",
    "        df: pd.DataFrame, \n",
    "        resample: str = \"1w\", \n",
    "        bins: int = 48, \n",
    "        range: Optional[Tuple[float, float]] = None,\n",
    "        clip: Optional[Tuple[float, float]] = None,\n",
    "        title: Optional[str] = None,\n",
    "        labels: Optional[dict] = None,\n",
    "):\n",
    "    if range is None:\n",
    "        df_n = df.replace(np.nan, 0)\n",
    "        range = (np.amin(df_n.values), np.amax(df_n.values))\n",
    "    df = pd.concat(\n",
    "        (pd.Series(np.histogram(group, bins=bins, range=range)[0], name=key)\n",
    "        for key, group in df.resample(resample, level=\"date\")),\n",
    "        axis=1\n",
    "    ).replace(0, np.nan)\n",
    "    df.index = np.linspace(*range, bins)\n",
    "    if clip is not None:\n",
    "        df = df.clip(*clip)\n",
    "    return px.imshow(\n",
    "        df, origin=\"lower\",\n",
    "        title=title or \"Weekly histogram of occupation per lot\",\n",
    "        labels=labels or {\"y\": \"occupation percentage\", \"x\": \"week\"},\n",
    "        color_continuous_scale=[\"#005\", \"#08f\", \"#8ff\", \"#fff\", \"#fff\"]\n",
    "    )\n",
    "\n",
    "# ignore values that are exactly zero or one\n",
    "#   as they are usually *bad data* (see below)\n",
    "plot_histogram(occupied.replace({0: np.nan, 1: np.nan}) * 100.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032bb2b6",
   "metadata": {},
   "source": [
    "So, starting end of March 2020, the most reported lot occupation is between 0 and 15%. The situation kind of normalizes in June and kind of returns in November. \n",
    "\n",
    "What are these small horizontal stripes you ask? And what happened in the beginning of 2018? \n",
    "\n",
    "The short 2018 outage is probably some internal server problem. You know, disk full, provider problems. There is no indication in the [commit history](https://github.com/offenesdresden/ParkAPI/commits/master?after=75ad91d24cb709c2e2065e2ff48a1628973926b9+174&branch=master).  \n",
    "\n",
    "To investigate the stripes, i'll spend few more Megabytes of generated javascript and look at a few lots in pariticular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d061c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lot_data(lot_id: str, filter: Optional[Callable] = None):\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        vertical_spacing=0.1,\n",
    "        shared_xaxes=True,\n",
    "        subplot_titles=[\"weekly occupation histogram\", \"number of free spaces per hour\"],\n",
    "    )\n",
    "    filter = filter or (lambda df: df)\n",
    "    df = filter(occupied[lot_id])\n",
    "    histo = plot_histogram(df * 100)\n",
    "    fig.add_trace(histo.data[0], row=1, col=1)\n",
    "    fig.add_trace(\n",
    "        filter(big_df[lot_id]).round().plot().data[0].update(showlegend=False), \n",
    "        row=2, col=1,\n",
    "    )\n",
    "    return fig.update_layout(\n",
    "        coloraxis=histo.layout.coloraxis, \n",
    "        title=f\"{lot_id} (capacity: {lot_infos[lot_id]['total']})\", height=700\n",
    "    )\n",
    "\n",
    "plot_lot_data(\"dresdenparkhausmitte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1264d877",
   "metadata": {},
   "source": [
    "Obviously, a horizontal stripe means that the free-spaces-counter stood still somehow. Except for the stripes at 0% occupation starting at the end of 2019. They are caused by the reported number of free spaces being larger than the reported lot capacity, which is (by the time of writing this article) 280. This garage must have decreased it's capacity in the meantime. It would be helpful if the recorded capacity would be published in the archive as well. Otherwise we must trust the maximum value which is 432 for this recording. However, if you zoom in at Oct 1st to 4th 2016 when this maximum was reached, you'll notice a completely unrealistic looking period of 200+ spaces. Also note that the little free peaks that occur each day around that period are upside-down within! It may still be possible that some real-life event has caused that but i find it more likely to be some digital mess-up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d7006",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lot_data(\"freiburgambahnhof\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb45b83",
   "metadata": {},
   "source": [
    "At first glance, the parking lot in Freibug looks much more lively compared to the one above. But please zoom in at the flat-line in winter 2016/17. There is obviously no real car activity but still the number of reported free spaces changes between zero and 62 each day in a super regular pattern reminding on opening hours. They only publish free places during opening hours. You know, that might make sense for drivers but it just makes interpreting the data harder. Since the outage in April 2018 they seem to be open 24/7 and data is published continously. Still, looking closely at some points it becomes hard to determine, for myself at least, if this is real car-in car-out activity. The patterns are so regular at times, e.g. from one weekend to the next, that i find it either creepy or not completely trustworthy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc6552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lot_data(\"ingolstadtreduittilly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be746ab1",
   "metadata": {},
   "source": [
    "This one's interesting. The number of cars in Ingolstadt seems to be growing. Although, once again, zooming in on the data reveals some strange jumps of the occupied spaces during the night from one week to the next which do not look like a reflection of 3d events. Or could this actually be gradual steps back towards working-life after the first lock-down? \n",
    "\n",
    "Changes to the capacity, whether real or digital, do affect the number of free spaces. And i start to realize that it's actually hard work to sample true *car-activity* just from the published number of free spaces. \n",
    " \n",
    "Gradients do have the same problem. I thought: *lets just look at the **difference** to the previous day* or something like that. This will at least mitigate the *opening-hours problem* and some other automatic or purely digital changes that the free-spaces counter might be subject to. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff1cc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = big_df[\"freiburgambahnhof\"]\n",
    "df = df[(df.index >= \"2020-01-01\") & (df.index < \"2020-06-01\")]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=4, cols=1,\n",
    "    vertical_spacing=0.02,\n",
    "    shared_xaxes=True,\n",
    "    subplot_titles=[\n",
    "        \"free spaces per hour\", \"difference to previous hour\", \n",
    "        \"difference to previous day\", \"difference to previous week\"\n",
    "    ],\n",
    ")\n",
    "fig.add_trace(df.plot().data[0], row=1, col=1)\n",
    "fig.add_trace(df.diff(1).plot().data[0], row=2, col=1)\n",
    "fig.add_trace(df.diff(24).plot().data[0], row=3, col=1)\n",
    "fig.add_trace(df.diff(24*7).plot().data[0], row=4, col=1)\n",
    "fig.update_layout(\n",
    "    height=1300, showlegend=False, \n",
    "    title=\"'freiburgambahnhof' free spaces and gradients (2020/01 - 2020/05)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0d9818",
   "metadata": {},
   "source": [
    "One can *see things*, still, it is hard to interpret this data automatically. \n",
    "\n",
    "Fine. I'm not a paid scientist, not even a scientist, but i want to scrutinize question #2 a bit: *Has anything in the parking behaviour significantly changed compared to before?* I mean, apart from the fact that there is less parking, anyways. So i'll try to look at the **occupation per hour-of-day**. In my previous parking post i found that there are some hints if occupation is driven by work & shopping activity or by more leisurely demands. \n",
    "\n",
    "But first i need to check the opening hours problem. If a lot does list zero free spaces at some point that translates to 1.0 in the `occupied` DataFrame and so i'll simply count the number of times that a lot has full occupation for each hour of day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020659c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_df = pd.concat([\n",
    "    (occupied[occupied.index.hour == hour] == 1).astype(int).sum()\n",
    "    for hour in range(24)\n",
    "], axis=1)\n",
    "zero_df.columns.rename(\"hour of day\", inplace=True)\n",
    "zero_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bbb913",
   "metadata": {},
   "source": [
    "Lets see. Some Dresden lots seem to be particularily busy during the day but that could also be because of the lot occupation being too small at periods. All the **Lübeck** lots and one in Freiburg do obviously publish zero free spaces when closed, so that's the ones to be careful about when calculating the occupation per hour. Though we also have seen previously that `freiburgambahnhof` did the same until 2018 and `freiburgmartinstor` and `freiburgzaehringertor` do look similar.\n",
    "\n",
    "Plotting the occupation data of the Lübeck lots hints at another problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdb14e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = occupied.loc[:, occupied.columns.map(lambda c: c.startswith(\"luebeck\"))]\n",
    "(df[(df.index >= \"2018-03-01\") & (df.index < \"2018-03-08\")] * 100).round().plot(\n",
    "    title=\"Occupation in Lübeck lots (March 2018)\",\n",
    "    labels={\"value\": \"occupation %\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad92656",
   "metadata": {},
   "source": [
    "First it looks like the lots open at 6:00 and close at 22:00 but there are these little edges at the corners. It's more likely they open at 6:30 and close at 20:30 or 21:30 but the final value is lost in the average bucketing of 1-hour-steps done in the beginning. Well, if they are closed, their data does not contribute to the *leisure activity* anyways so i'll simply cut off all lots that have a zero-count of more than 400 at conservative times that are safe to assume, before 7:00 and after 20:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d002ef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupied_open = occupied.copy()\n",
    "for lot_id in zero_df[zero_df[0] > 400].index:\n",
    "    df = occupied_open.loc[:, lot_id]\n",
    "    occupied_open.loc[:, lot_id] = df[(df.index.hour >= 7) & (df.index.hour <= 20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc67178",
   "metadata": {},
   "source": [
    "Just to make sure i plot the sample range again for all lots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd96670",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = occupied_open\n",
    "(df[(df.index >= \"2018-03-01\") & (df.index < \"2018-03-08\")] * 100).round().plot(\n",
    "    title=\"Occupation during opening times (March 2018)\",\n",
    "    labels={\"value\": \"occupation %\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034bc481",
   "metadata": {},
   "source": [
    "As far as i can determine, there are no regular hard edges any more. So then gimme that **occupation per hour-of-day** plot, individually for every year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6a5ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hours_year_group(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"year\"] = df.index.year\n",
    "    df[\"hour\"] = df.index.hour\n",
    "    return (\n",
    "        df.reset_index().set_index([\"date\", \"year\", \"hour\"])\n",
    "        .unstack(\"year\")\n",
    "        .groupby(level=\"hour\").mean()\n",
    "        .groupby(level=\"year\", axis=1).mean()\n",
    "    )\n",
    "    \n",
    "(hours_year_group(occupied_open) * 100).plot(\n",
    "    title=\"mean occupation per hour of day\",\n",
    "    labels={\"value\": \"occupation %\"},\n",
    "    color_discrete_sequence=[\"#aa4\", \"#4a4\", \"#4aa\", \"#48a\", \"#f00\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3462447",
   "metadata": {},
   "source": [
    "Amazing, isn't it? No, not really. And the bump at 20:00 is not making much sense. Let's plot the mean for each city individually: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b89f3e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def per_city_plot(occupied_open: pd.DataFrame, title: Optional[str] = None):\n",
    "    fig = make_subplots(\n",
    "        rows=len(CITIES), cols=1,\n",
    "        vertical_spacing=0.02,\n",
    "        shared_xaxes=True,\n",
    "        subplot_titles=CITIES,\n",
    "    )\n",
    "    for i, city in enumerate(CITIES):\n",
    "        df = occupied_open.loc[:, occupied_open.columns.map(lambda c: c.startswith(city.lower()))]\n",
    "        for trace in (hours_year_group(df) * 100).round().plot(\n",
    "            labels={\"value\": \"occupation %\"},\n",
    "            color_discrete_sequence=[\"#aa4\", \"#4a4\", \"#4aa\", \"#48a\", \"#f00\"],\n",
    "        ).data:\n",
    "            if i != 0:\n",
    "                trace.showlegend = False\n",
    "            fig.add_trace(trace, row=i+1, col=1)\n",
    "    fig.update_layout(\n",
    "        title=title or \"mean occupation per hour of day\", height=1000,\n",
    "    ).show()\n",
    "    \n",
    "per_city_plot(occupied_open)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984328a5",
   "metadata": {},
   "source": [
    "Obviously, **Lübeck** has parking lots that have closed even before 20:00 at some point. Apart from that, the Lübeck plot actually shows something i am looking for: Through working hours the occupation rate is similar to the years before 2020 while the evenings are certainly less occupied. \n",
    "\n",
    "**Freiburg** also shows this little peak at 20:00 which is most likely caused by the *closing hours problem* and not by party goers.\n",
    "\n",
    "**Dresden** shows a different picture. Seems like in 2020 more cars are simply left standing in the garage during the night. Dresden is quite a nice town with a lot of cool places to visit during the night--if there is no emergency decree, that is.\n",
    "\n",
    "And as seen previously, **Ingolstadt**'s number of parked cars is growing over the years. In 2016 people stayed out longer compared to the other years. \n",
    "\n",
    "Okay, well, please be aware! These are all just my assumptions. To proof anything, each parking lot has to be inspected individually. That is not what i want to do in this post. It has already a couple of Megabytes of javascript in it. I'll stick with these average statistics but remember, if the river is half a meter deep on average, that does not mean that the cow is not going to drown when crossing it.\n",
    "\n",
    "Finally, i'll just repeat the above plot but for two particular weekdays: Wednesday and Sunday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bb35b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_city_plot(\n",
    "    occupied_open[occupied_open.index.map(lambda d: d.weekday() == 2)],\n",
    "    title=\"mean occupation per hour of day on Wednesdays\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ab4935",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_city_plot(\n",
    "    occupied_open[occupied_open.index.map(lambda d: d.weekday() == 6)],\n",
    "    title=\"mean occupation per hour of day on Sundays\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851d8b12",
   "metadata": {},
   "source": [
    "Thanks for reading! \n",
    "\n",
    "Some applause to the *parkenDD* people and, really, don't drink and drive! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
